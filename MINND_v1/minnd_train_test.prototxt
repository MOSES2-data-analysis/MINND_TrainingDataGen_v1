name: "MINND_v1"
# Data layer for storing input images
# These images are the results of the MOSES forward model
layer{
	name: 	"minnd_input"
	type: 	"Data"
	top:	"input"
	
	include {
		phase: TEST
	}
	
	# Scale from [0,255] to [0,1]
	transform_param {
    	scale: 0.00390625
	}
	
	data_param {
		source: "/minnd/testing_data/minnd_input_LMBD"
		batch_size: 100
		backend: LMDB
	}
}

# Data layer for storing truth images.
# These images are the input to the MOSES forward model
layer{
	name: 	"minnd_truth"
	type: 	"Data"
	top:	"truth"
	
	include {
		phase: TEST
	}
	
	# Scale from [0,255] to [0,1]
	transform_param {
    	scale: 0.00390625
	}
	
	data_param {
		source: "/minnd/testing_data/minnd_truth_LMBD"
		batch_size: 100
		backend: LMDB
	}
}

layer{
	name: 	"minnd_input"
	type: 	"Data"
	top:	"input"
	
	include {
		phase: TRAIN
	}
	
	# Scale from [0,255] to [0,1]
	transform_param {
    	scale: 0.00390625
	}
	
	data_param {
		source: "/minnd/training_data/minnd_input_LMBD"
		batch_size: 100
		backend: LMDB
	}
}

# Data layer for storing truth images.
# These images are the input to the MOSES forward model
layer{
	name: 	"minnd_truth"
	type: 	"Data"
	top:	"truth"
	
	include {
		phase: TRAIN
	}
	
	# Scale from [0,255] to [0,1]
	transform_param {
    	scale: 0.00390625
	}
	
	data_param {
		source: "/minnd/training_data/minnd_truth_LMBD"
		batch_size: 100
		backend: LMDB
	}
}

# First Convolutional Layer
layer {
	name: 	"conv1"
	type: 	"Convolution"
	bottom: "input"
	top: 	"conv1"
	
	# learning rate and decay multipliers for the filters
	param { lr_mult: 1 decay_mult: 1 }
	# learning rate and decay multipliers for the biases
	param { lr_mult: 2 decay_mult: 0 }
  
	convolution_param {
		num_output:		24
		kernel_size:	3
		stride:			1

		weight_filler {
		  type: "gaussian" # initialize the filters from a Gaussian
		  std: 0.1        # distribution with stdev 0.01 (default mean: 0)
		}
		bias_filler {
		  type: "constant" # initialize the biases to zero (0)
		  value: 0
		}
    }
}

# Max pooling layer
layer {
	name: "pool1"
	type: "Pooling"
	bottom: "conv1"
	top: "pool1"
	pooling_param {
		pool: MAX
		kernel_size: 2
		stride: 1
	}
}

# Second Convolutional Layer
#layer {
#	name: 	"conv2"
#	type: 	"Convolution"
#	bottom: "conv1"
#	top: 	"conv2"
	
	# learning rate and decay multipliers for the filters
#	param { lr_mult: 1 decay_mult: 1 }
  	# learning rate and decay multipliers for the biases
#  	param { lr_mult: 2 decay_mult: 0 }
#  
#	convolution_param {	
#	  	num_output:		48
#	  	kernel_size:	2
#	  	stride:			1
#	  
#	  	weight_filler {
#		  	type: "gaussian" # initialize the filters from a Gaussian
#		  	std: 0.1       # distribution with stdev 0.01 (default mean: 0)
#		}
#		bias_filler {
#		  	type: "constant" # initialize the biases to zero (0)
#		  	value: 0
#		}
#    }
#}

# First fully-connected layer
layer {
	name: "ip1"
	type: "InnerProduct"
	bottom: "pool1"
	top: "ip1"
	
	# learning rate and decay multipliers for the filters
	param { lr_mult: 1 decay_mult: 1 }
  	# learning rate and decay multipliers for the biases
  	param { lr_mult: 2 decay_mult: 0 }
  	
	inner_product_param {
		num_output: 64
	
	  	weight_filler {
		  	type: "gaussian" # initialize the filters from a Gaussian
		  	std: 0.1       # distribution with stdev 0.01 (default mean: 0)
		}
		bias_filler {
		  	type: "constant" # initialize the biases to zero (0)
		  	value: 0
		}
    }
}

# Rectified Linear Unit layer
layer {
  name: "relu1"
  type: "ReLU"
  bottom: "ip1"
  top: "ip1"
}

# Second fully-connected layer
layer {
	name: "ip2"
	type: "InnerProduct"
	bottom: "ip1"
	top: "ip2"
	
	# learning rate and decay multipliers for the filters
	param { lr_mult: 1 decay_mult: 1 }
  	# learning rate and decay multipliers for the biases
  	param { lr_mult: 2 decay_mult: 0 }
  	
	inner_product_param {
		num_output: 128	
	
	  	weight_filler {
		  	type: "gaussian" # initialize the filters from a Gaussian
		  	std: 0.1       # distribution with stdev 0.01 (default mean: 0)
		}
		bias_filler {
		  	type: "constant" # initialize the biases to zero (0)
		  	value: 0
		}
    }
}

# Rectified Linear Unit layer
layer {
  name: "relu2"
  type: "ReLU"
  bottom: "ip2"
  top: "ip2"
}

# Third fully-connected layer
layer {
	name: "ip3"
	type: "InnerProduct"
	bottom: "ip2"
	top: "ip3"
	
	# learning rate and decay multipliers for the filters
	param { lr_mult: 1 decay_mult: 1 }
  	# learning rate and decay multipliers for the biases
  	param { lr_mult: 2 decay_mult: 0 }
  	
	inner_product_param {
		num_output: 48	
	
	  	weight_filler {
		  	type: "gaussian" # initialize the filters from a Gaussian
		  	std: 0.1       # distribution with stdev 0.01 (default mean: 0)
		}
		bias_filler {
		  	type: "constant" # initialize the biases to zero (0)
		  	value: 0
		}
    }
}

# Reshape to be the same dimensions as the truth
layer {
	name: "reshape_ip3"
	type: "Reshape"
	bottom: "ip3"
	top: "reshape_ip3"
	reshape_param {
		shape {
			dim: 0  # copy the dimension from below
			dim: 3	
			dim: 4
			dim: 4
		}
	}
}

#layer {
#	name: "reshape_truth"
#	type: "Reshape"
#	bottom: "truth"
#	top: "reshape_truth"
#	reshape_param {
#		shape {
#			dim: 0  # copy the dimension from below
#			dim: 1	
#			dim: 48
#			dim: -1 # infer it from the other dimensions
#		}
#	}
#}

layer {
	name: "loss"
	type: "EuclideanLoss"
	bottom: "truth"
	bottom: "reshape_ip3"
	top: "loss"	
}














