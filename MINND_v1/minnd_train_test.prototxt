name: "MINND_v1"
# Data layer for storing input images
# These images are the results of the MOSES forward model
layer{
	name: 	"minnd_input"
	type: 	"Data"
	top:	"input"
	top:	"input_label"
	
	include {
		phase: TEST
	}
	
	# Scale from [0,255] to [0,1]
	transform_param {
    	scale: 0.00390625
	}
	
	data_param {
		source: "/minnd/lmdb/test/input/"
		batch_size: 100
		backend: LMDB
	}
}



# Data layer for storing truth images.
# These images are the input to the MOSES forward model
layer{
	name: 	"minnd_truth"
	type: 	"Data"
	top:	"truth"
	top:	"truth_label"
	
	include {
		phase: TEST
	}
	
	# Scale from [0,255] to [0,1]
	transform_param {
    	scale: 0.00390625
	}
	
	data_param {
		source: "/minnd/lmdb/test/truth/"	
		batch_size: 100
		backend: LMDB
	}
}



layer{
	name: 	"minnd_input"
	type: 	"Data"
	top:	"input"
	top:	"input_label"
	
	include {
		phase: TRAIN
	}
	
	# Scale from [0,255] to [0,1]
	transform_param {
    	scale: 0.00390625
	}
	
	data_param {
		source: "/minnd/lmdb/train/input/"
		batch_size: 100
		backend: LMDB
	}
}


# Data layer for storing truth images.
# These images are the input to the MOSES forward model
layer{
	name: 	"minnd_truth"
	type: 	"Data"
	top:	"truth"
	top:	"truth_label"
	
	include {
		phase: TRAIN
	}
	
	# Scale from [0,255] to [0,1]
	transform_param {
    	scale: 0.00390625
	}
	
	data_param {
		source: "/minnd/lmdb/train/truth/"
		batch_size: 100
		backend: LMDB
	}
}

layer {
	name: "silence1"
	type: "Silence"
	bottom: "input_label"
}

layer {
	name: "silence2"
	type: "Silence"
	bottom: "truth_label"
}

# First Convolutional Layer
layer {
	name: 	"conv1"
	type: 	"Convolution"
	bottom: "input"
	top: 	"conv1"
	
	# learning rate and decay multipliers for the filters
	param { lr_mult: 1 decay_mult: 1 }
	# learning rate and decay multipliers for the biases
	param { lr_mult: 2 decay_mult: 0 }
  
	convolution_param {
		num_output:		32
		kernel_size:	5
		stride:			1

		weight_filler {
		  type: "gaussian" # initialize the filters from a Gaussian
		  std: 0.1        # distribution with stdev 0.01 (default mean: 0)
		}
		bias_filler {
		  type: "constant" # initialize the biases to zero (0)
		  value: 0
		}
    }
}

# Max pooling layer
layer {
	name: "pool1"
	type: "Pooling"
	bottom: "conv1"
	top: "pool1"
	pooling_param {
		pool: MAX
		kernel_size: 2
		stride: 1
	}
}

# Second Convolutional Layer
layer {
	name: 	"conv2"
	type: 	"Convolution"
	bottom: "pool1"
	top: 	"conv2"
	
	# learning rate and decay multipliers for the filters
	param { lr_mult: 1 decay_mult: 1 }
  	# learning rate and decay multipliers for the biases
  	param { lr_mult: 2 decay_mult: 0 }
  
	convolution_param {	
	  	num_output:		128
	  	kernel_size:	2
	  	stride:			1
	  
	  	weight_filler {
		  	type: "gaussian" # initialize the filters from a Gaussian
		  	std: 0.1       # distribution with stdev 0.01 (default mean: 0)
		}
		bias_filler {
		  	type: "constant" # initialize the biases to zero (0)
		  	value: 0
		}
    }
}

# Max pooling layer
layer {
	name: "pool2"
	type: "Pooling"
	bottom: "conv2"
	top: "pool2"
	pooling_param {
		pool: MAX
		kernel_size: 2
		stride: 1
	}
}

# First fully-connected layer
layer {
	name: "ip1"
	type: "InnerProduct"
	bottom: "pool2"
	top: "ip1"
	
	# learning rate and decay multipliers for the filters
	param { lr_mult: 1 decay_mult: 1 }
  	# learning rate and decay multipliers for the biases
  	param { lr_mult: 2 decay_mult: 0 }
  	
	inner_product_param {
		num_output: 192
	
	  	weight_filler {
		  	type: "gaussian" # initialize the filters from a Gaussian
		  	std: 0.1       # distribution with stdev 0.01 (default mean: 0)
		}
		bias_filler {
		  	type: "constant" # initialize the biases to zero (0)
		  	value: 0
		}
    }
}

# Rectified Linear Unit layer
layer {
  name: "relu1"
  type: "ReLU"
  bottom: "ip1"
  top: "ip1"
}

# Second fully-connected layer
layer {
	name: "ip2"
	type: "InnerProduct"
	bottom: "ip1"
	top: "ip2"
	
	# learning rate and decay multipliers for the filters
	param { lr_mult: 1 decay_mult: 1 }
  	# learning rate and decay multipliers for the biases
  	param { lr_mult: 2 decay_mult: 0 }
  	
	inner_product_param {
		num_output: 256	
	
	  	weight_filler {
		  	type: "gaussian" # initialize the filters from a Gaussian
		  	std: 0.1       # distribution with stdev 0.01 (default mean: 0)
		}
		bias_filler {
		  	type: "constant" # initialize the biases to zero (0)
		  	value: 0
		}
    }
}



# Rectified Linear Unit layer
layer {
  name: "relu2"
  type: "ReLU"
  bottom: "ip2"
  top: "ip2"
}

# Third fully-connected layer
layer {
	name: "ip3"
	type: "InnerProduct"
	bottom: "ip2"
	top: "ip3"
	
	# learning rate and decay multipliers for the filters
	param { lr_mult: 1 decay_mult: 1 }
  	# learning rate and decay multipliers for the biases
  	param { lr_mult: 2 decay_mult: 0 }
  	
	inner_product_param {
		num_output: 64	
	
	  	weight_filler {
		  	type: "gaussian" # initialize the filters from a Gaussian
		  	std: 0.1       # distribution with stdev 0.01 (default mean: 0)
		}
		bias_filler {
		  	type: "constant" # initialize the biases to zero (0)
		  	value: 0
		}
    }
}

# Reshape to be the same dimensions as the truth
layer {
	name: "reshape_ip3"
	type: "Reshape"
	bottom: "ip3"
	top: "reshape_ip3"
	reshape_param {
		shape {
			dim: 0  # copy the dimension from below
			dim: 1	
			dim: 8
			dim: 8
		}
	}
}

layer {
  name: "slice_truth"
  type: "Slice"
  bottom: "truth"
  ## Example of label with a shape N x 3 x 1 x 1
  top: "truth0"
  top: "truth1"
  top: "truth2"
  slice_param {
    axis: 1
    slice_point: 1
    slice_point: 2
  }
}

layer {
	name: "silence3"
	type: "Silence"
	bottom: "truth1"
}

layer {
	name: "silence4"
	type: "Silence"
	bottom: "truth2"
}

layer {
	name: "loss"
	type: "EuclideanLoss"
	bottom: "truth0"
	bottom: "reshape_ip3"
	top: "loss"	
}














